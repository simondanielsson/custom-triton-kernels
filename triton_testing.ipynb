{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOtCb9OZ29Ozu6zEbRUqyio",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simondanielsson/triton-kernels/blob/main/triton_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOnifssYenFe",
        "outputId": "2a84a1d0-fd49-4fa8-a54d-1a195eb476c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\n",
            "Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton\n",
            "Successfully installed triton-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install triton"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def timing(func):\n",
        "\n",
        "  def wrapper(*args, **kwargs):\n",
        "      t0 = time.perf_counter()\n",
        "      result = func(*args, **kwargs)\n",
        "      print(f\"Ran in {time.perf_counter() - t0}s\")\n",
        "      return result\n",
        "\n",
        "  return wrapper"
      ],
      "metadata": {
        "id": "Ga07ON4tfuiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0pVXHn1kM6X",
        "outputId": "9959469f-2b66-49d9-af9a-3b9429f94c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d51b2f65e70>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a GPU kernel in Triton.\n",
        "# Different instances of this\n",
        "# function may run in parallel.\n",
        "@triton.jit\n",
        "def add_kernel(x_ptr, y_ptr, output_ptr, n_elements: tl.constexpr, BLOCK_SIZE: tl.constexpr,):\n",
        "  \"\"\"\n",
        "  x_ptr and y_ptr: input pointers\n",
        "  output_ptr: output pointer\n",
        "  n_elements: size of the inputs so we don't access prohibited memory\n",
        "  BLOCK_SIZE: Number of elements each program should process. constexpr required to be used as shape value\n",
        "  \"\"\"\n",
        "  # get grid index associated with this kernel instance, here from 0th dim\n",
        "  # We use a 1D launch grid so axis is 0.\n",
        "  pid = tl.program_id(0)\n",
        "\n",
        "  # This program will process inputs that are offset from the initial data.\n",
        "  # For instance, if you had a vector of length 256 and block_size of 64, the programs\n",
        "  # would each access the elements [0:64, 64:128, 128:192, 192:256].\n",
        "  # Note that offsets is a list of pointers:\n",
        "  block_start = pid * BLOCK_SIZE\n",
        "  offsets = tl.arange(0, BLOCK_SIZE) + block_start\n",
        "\n",
        "  # Create a mask to guard memory operations against out-of-bounds accesses.\n",
        "  mask = offsets < n_elements\n",
        "\n",
        "  # fetch the slices of the inputs using pointer arithmetics rather than indexing operators\n",
        "  x = tl.load(x_ptr + offsets, mask=mask)\n",
        "  y = tl.load(y_ptr + offsets, mask=mask)\n",
        "\n",
        "  # store the values for the relevant indices back to the output\n",
        "  tl.store(output_ptr + offsets, x + y, mask=mask)\n",
        "\n",
        "def add(x: torch.Tensor, y: torch.Tensor):\n",
        "    # We need to preallocate the output.\n",
        "    output = torch.empty_like(x)\n",
        "    assert x.is_cuda and y.is_cuda and output.is_cuda\n",
        "\n",
        "    n_elements = output.numel()\n",
        "\n",
        "    # The SPMD launch grid denotes the number of kernel instances that run in parallel.\n",
        "    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\n",
        "    # In this case, we use a 1D grid where the size is the number of blocks:\n",
        "\n",
        "    # Ceiling division ensures the grid always contains enough indices to stride through the full dimension of the input\n",
        "    # E.g.: if n_elements = 1024 and BLOCK_SIZE=64, then grid is (8,). 8 program indexes are enough to cover the full 1024 length of the input using strides)\n",
        "    # It also ensure that if BLOCK_SIZE > n_elements, e.g. block of 2048 with input of 1024 then grid is (1,) since it always rounds 0.00x to 1, i.e. we compute in only one block (one instance)\n",
        "    # if input length an block size is not divisible (i.e. not returning an int), then we must make sure to mask the output to not access outside data in the last kernel instance (due to last block covering more than the input)\n",
        "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n",
        "\n",
        "    # NOTE:\n",
        "    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\n",
        "    #  - Don't forget to pass meta-parameters as keywords arguments.\n",
        "    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=16)\n",
        "\n",
        "    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\n",
        "    # running asynchronously at this point.\n",
        "    return output\n",
        "\n",
        "def simple_add(x, y):\n",
        "  return x + y\n",
        "\n",
        "def test_add():\n",
        "  vector_size = 512\n",
        "\n",
        "  size = 98432\n",
        "  x = torch.rand(vector_size, device='cuda')\n",
        "  y = torch.rand(vector_size, device='cuda')\n",
        "  output_torch = timing(simple_add)(x, y)\n",
        "  output_triton = timing(add)(x, y)\n",
        "  print(f'The maximum difference between torch and triton is '\n",
        "      f'{torch.max(torch.abs(output_torch - output_triton))}')\n",
        "\n",
        "test_add()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q_Sb3D-hvRa",
        "outputId": "567cce5c-c9ef-4846-c5b6-97ff1a71bd3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ran in 4.1091000184678705e-05s\n",
            "Ran in 0.08164957000008144s\n",
            "The maximum difference between torch and triton is 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JQzuWjLhoKz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "def naive_softmax(x):\n",
        "  \"\"\"x is of size MxN, M rows and N columns\"\"\"\n",
        "  # find the max along each row\n",
        "  # read  MN elements ; write M elements\n",
        "  x_max = x.max(dim=1)[0] # get all row values by accessing 0 index, as a single row vector\n",
        "\n",
        "  # transpose to get max for each row. No reading for transpose\n",
        "  x_max_per_row = x_max[:, None]\n",
        "\n",
        "  # read MN + M elements ; write MN elements\n",
        "  x_scaled = x - x_max_per_row\n",
        "\n",
        "  # read  MN elements ; write MN elements\n",
        "  numerator = torch.exp(x_scaled)\n",
        "  # sum along the rows (giving a single flat row vector), and transpose to get them as one value per row\n",
        "  # read  MN elements ; write M  elements\n",
        "  denominator = numerator.sum(dim=1)[:, None]\n",
        "\n",
        "  # read MN elements (numerator) + M elements (denominator), write MN elements\n",
        "  result = numerator / denominator\n",
        "\n",
        "  # Total reads: 5MN + 2M\n",
        "  # Total writes: 3MN + 2M\n",
        "  return result\n",
        "\n",
        "@triton.jit\n",
        "def softmax_kernel_parallelize_rows(X, Y, stride_xm, stride_xn, stride_ym, stride_yn, n_elements: tl.constexpr):\n",
        "    # stride_xm helps us get to the right row using strides (where is row 3? in pid * stride_xm)\n",
        "    # stride_xn helps us to get the correct elements in the row once we have identified it\n",
        "     # (where is element 1-4 of row 3? in [0, 1, 2, 3] if rows stored contigously; if columns stored contiguously then it could be [0, 2, 4, 6] if we have 2 columns)\n",
        "     # Typically, stride_xn == 1, but not necessarily depending on how data is stored\n",
        "\n",
        "\n",
        "    # get row index: grid is flat\n",
        "    pid = tl.program_id(0)\n",
        "\n",
        "    # find input indexes to use for this program\n",
        "        # n_elements is the number of columns to sum over for this row, but since the input is flattened we need to stride appropriately\n",
        "        # example: pid=1, n_elements = 6 => [6, 7, 8, 9, 10, 11]\n",
        "        # typically, if the matrix is stored in row-major order (rows are stored in contiguous memory), then stride_xn is 1 (i.e. each row element is next to each other)\n",
        "    offsets = tl.arange(0, n_elements) * stride_xn + pid * stride_xm\n",
        "\n",
        "    # no need for masking since we are reading full row of data\n",
        "    x = tl.load(X + offsets)\n",
        "\n",
        "    # Compute softmax within the block\n",
        "    row_max = tl.max(x, axis=0) # axis=0 because input is flat (a row)\n",
        "    exp_row = tl.exp(x - row_max)\n",
        "    sum_exp = tl.sum(exp_row, axis=0) # axis=0 because input is flat (a row)\n",
        "\n",
        "    # Normalize the output\n",
        "    result = exp_row / sum_exp\n",
        "\n",
        "    # Store the result in global memory\n",
        "    output_offsets = tl.arange(0, n_elements) * stride_yn + pid * stride_ym\n",
        "    tl.store(Y + output_offsets, result)\n",
        "1\n",
        "def softmax_triton_parallelize_rows(X):\n",
        "    # Define the input tensor and the output\n",
        "    Y = torch.empty_like(X)\n",
        "\n",
        "    # Kernel launch parameters, flattened matrix\n",
        "    n_rows, n_cols = X.shape\n",
        "    grid = (n_rows,)\n",
        "\n",
        "    # Launch the Triton kernel\n",
        "    # stride says how many indexes you have to jump to get to the next element along the dimension.\n",
        "    # E.g. to get to the next element if flatting it as a row (dim 0), we need to jump the entire length of one row to get to the next row.\n",
        "    #   stride(1) is then typically 1, since it requires one step in the columns to go to the next column.\n",
        "    # Y pointer will be populated\n",
        "    softmax_kernel_parallelize_rows[grid](\n",
        "        X,\n",
        "        Y,\n",
        "        X.stride(0),\n",
        "        X.stride(1),\n",
        "        Y.stride(0),\n",
        "        Y.stride(1),\n",
        "        # length of each input for each kernel instance (program)\n",
        "        n_cols,\n",
        "    )\n",
        "    return Y\n",
        "\n",
        "# Usage example\n",
        "size = 128\n",
        "x = torch.rand((size, size), device='cuda')\n",
        "\n",
        "print(f\"diff naive vs torch: {torch.max(torch.abs(naive_softmax(x) - torch.softmax(x, 1)))}\")\n",
        "print(f\"diff naive vs triton parallelize rows: {torch.max(torch.abs(naive_softmax(x) - softmax_triton_parallelize_rows(x)))}\")\n",
        "print(f\"diff torch vs triton parallelize rows: {torch.max(torch.abs(torch.softmax(x, 1) - softmax_triton_parallelize_rows(x)))}\")\n",
        "#print(softmax_triton(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPNjobY0fEeI",
        "outputId": "8c51c71b-4849-474c-95c6-2861a59bd712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "diff naive vs torch: 1.862645149230957e-09\n",
            "diff naive vs triton parallelize rows: 2.7939677238464355e-09\n",
            "diff torch vs triton parallelize rows: 2.7939677238464355e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: implement parallelized on columns as well, if possible. Or fused kernel"
      ],
      "metadata": {
        "id": "tPjlAHOBy1Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from triton.runtime import driver\n",
        "device = torch.cuda.current_device()\n",
        "properties = driver.active.utils.get_device_properties(device)\n",
        "NUM_SM = properties[\"multiprocessor_count\"]\n",
        "NUM_REGS = properties[\"max_num_regs\"]\n",
        "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
        "WARP_SIZE = properties[\"warpSize\"]\n",
        "target = triton.runtime.driver.active.get_current_target()"
      ],
      "metadata": {
        "id": "AeXHOZAytmKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vMp9o0_6gEVj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}